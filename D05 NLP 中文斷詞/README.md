# Day5 NLP 中文斷詞
* 在機器學習中是透過向量來訓練，因此要將文章轉為向量呈現，必須把文章斷為字詞來代表該文章
* 英文斷詞相對簡單，透過空格斷詞即可
* 而中文則是所有詞都串連在一起，會有下列的可能性

|句子|斷詞結果|
|-|-|
|全台大停電|全/台大/停電|
|全台大停電|全台/大/停電|
|全台大停電|全/台/大停電|

* 因此，有些方法幫助電腦學習中文斷詞

## 斷詞方法
* 斷詞的主流方法
    * 基於詞典的斷詞法：按照策略將待匹配的字串和已建立好的詞典進行匹配
    * 基於統計的機器學習算法：HMM, Conditional Random Field(CRF), SVM, etc
    * 基於深度學習的算法：雙向 LSTM 模型
* 目前主流的中文斷詞 "結巴" 是基於傳統機器學習算法的斷詞演算法

## 結巴斷詞介紹
* 結巴斷詞主要分為：
    * 針對存在於字典的字詞：
        * 根據字典產生 Trie Tree(字典樹、字首樹、前綴樹)
        * 根據 Trie Tree 建立給定輸入句的 DAG (有向無環圖)
        * 使用動態規劃 (Dynamic Programming) 來找出最大機率路徑，此路徑為基於詞頻最大的斷詞結果
    * 針對不存在於字典的字詞：
        * 使用隱馬可夫模型 (HMM) 與維特比演算法 (Viterbi) 進行斷詞辨識

### 字典樹 (Trie Tree)
* Trie Tree 基於一個字典建構，在結巴中字典為 dict.txt，內含兩萬多個詞，包含詞、詞頻、詞性 (結巴作者根據中國人民日報語料訓練)
* 當數個詞語前面的字相同 (即有相同前綴)，就可以使用 Trie Tree 來儲存

### 有向無環圖 (DAG)
* 給一個待斷詞的句子，根據由字典生成的 Trie Tree 查找來生成有向無環圖
* 根據字典查詢，列舉出根據字典所有可能的句子切分
* 結巴將句子中某個詞的開始位置作為 dictionary 的 key，結尾位置 (多個) 構成的 list 作為 dictionary 的值
ex: "我愛自然語言" 的 DAG:
    ```
    {0: [0], 1: [1], 2:[2, 3], 3:[3], 4:[4, 5], 5: [5]}
    ```
    * `0:[0]` 表示在 dict.txt 建立的 Trie Tree 中以 "我" 為開頭的詞只有一個
    * `2: [2, 3]` 表示在 dict.txt 建立的 Trie Tree 中以 "自" 為開頭的詞有：a. "自"; b. "自然"
    * 最後根據動態規劃，找出基於 dict.txt 中詞頻最大機率路徑得到最後斷詞結果

### 馬可夫模型
* 一種具有狀態的隨機過程，從目前狀態 s 轉移到下一個狀態 s' 的機率由 P(s'|s) 來決定 (在 s 的前提下 s’ 發生的機率)，這個狀態之轉移機率並不會受到狀態以外的因素所影響，因此與時間無關
* 一階馬可夫模型：當前狀態只與前一個狀態有關 P(Xi|Xi-1)
* m 階馬可夫模型: 當前狀態可能受前 m 個狀態所影響 P(Xi|Xi-1, Xi-2, ...., Xi-m)

#### 狀態轉移矩陣
* 當由一個狀態要轉移到下一個狀態時，會有相對應的轉移機率，而所有可能的轉移機率構成的矩陣即為狀態轉移矩陣 (對於有 M 個狀態的一階馬可夫模型，因為任何一個狀態都有可能是所有狀態的下一個轉移狀態，因此會有 MxM 個狀態的轉移矩陣)

#### 初始機率向量 (PI 向量)
* 初始化模型，會需要當前的狀態情況，而表示此狀態的向量即為初始機率向量

ex: 一階馬可夫天氣模型 (假設有三種天氣狀態: Sunny, Cloudy, Rainy)
![](https://github.com/myps6415/NLP-marathon/blob/main/D05%20NLP%20中文斷詞/Markov_example.png?raw=true)

* 狀態轉移矩陣
昨天是晴天，則今天是晴天的機率為 0.5，陰天的機率為 0.375，雨天的機率為 0.125
![](https://github.com/myps6415/NLP-marathon/blob/main/D05%20NLP%20中文斷詞/Markov_example2.png?raw=true)

* 初始機率向量
第一天是晴天的機率為100%

狀態：Sunny, Cloudy, Rainy
PI 向量：初始化時每一個狀態的機率
狀態轉移矩陣：給定前一天狀態下，當前的天氣狀態機率

### 隱馬可夫模型
* 在某些情況下，我們無法直接觀測到狀態資訊 (ex: 晴天、多雲、雨天)，但我們可以藉由其他可察覺的*觀察狀態*
* 例如我們可以觀察水草的狀態 (ex:乾燥、濕潤、潮濕)，來推測實際狀態的機率為何
ex: 天氣狀態 (Sunny, Rainy), 觀察狀態 (Walk, Shop, Wet)
![](https://github.com/myps6415/NLP-marathon/blob/main/D05%20NLP%20中文斷詞/HMM_example.png?raw=true)

    * 隱藏狀態：系統的真實狀態 (Sunny, Rainy)
    * 觀察狀態：在過程中 "可視" 的狀態
    * PI 向量：初始化時隱藏狀態的機率
    * 狀態轉移矩陣：隱藏狀態到另外一個隱藏狀態的機率
    * 發射矩陣：隱藏狀態觀察到某一個觀察狀態的機率
帶入機率計算結果
![](https://github.com/myps6415/NLP-marathon/blob/main/D05%20NLP%20中文斷詞/HMM_example2.png?raw=true)

若有一個人連續觀察到三天水草都是乾燥的 (Dry)，則這三天的天氣機率為何？

觀察狀態：Dry, Dry, Dry

隱藏狀態：
Sunny, Sunny, Sunny: 0.4 * 0.6 * 0.6 * 0.6 * 0.6 * 0.6 = 0.031104  
Rainy, Sunny, Sunny: 0.6 * 0.1 * 0.3 * 0.6 * 0.6 * 0.6 = 0.003888

這裡對於上述的機率來做個詳盡的解釋 (以第一個狀態 Sunny, Sunny, Sunny 為例):

a. 第一個 Sunny: Sunny 的 initial = 0.4，而由 Dry 的狀態下是 Sunny 的機率 = 0.6 => 0.4 * 0.6  
b. 第二個 Sunny: 前一天是 Sunny 而今天還是 Sunny 的機率 = 0.6，而由 Dry 的狀態下是 Sunny 的機率 = 0.6 => 0.6 * 0.6  
c. 第三個 Sunny: 前一天是 Sunny 而今天還是 Sunny 的機率 = 0.6，而由 Dry 的狀態下是 Sunny 的機率 = 0.6 => 0.6 * 0.6

因此最後 Sunny, Sunny, Sunny 在 Dry, Dry, Dry 的觀察狀態下的機率是 0.4 * 0.6 * 0.6 * 0.6 * 0.6 * 0.6 = 0.031104

### 維特比 (Viterbi)
* 使用動態規劃求解隱馬可夫模型預測，即使用動態規劃求解最大機率路徑
* 從 t=1 的時刻，開始計算在時刻 t 狀態 i 的所有路徑的最大概率，直到到達終點 t=T 時狀態為 i 的所有路徑的最大機率
![](https://github.com/myps6415/NLP-marathon/blob/main/D05%20NLP%20中文斷詞/Viterbi.jpeg?raw=true)
* Viterbi會在所有路徑中，找尋最大機率的路徑
* 圖中，所有路徑裡 A->D->G 的機率如果最大，則 Viterbi 即為選取此路徑

### 未知詞斷詞
